{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "---\n",
    "\n",
    "*You can watch a [video](https://youtu.be/4wuIOcD1LLI) of this lesson if you prefer.*\n",
    "\n",
    "---\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "- [Introduction](#Introduction)\n",
    "- [The RNN Architecture](#The-RNN-Architecture)\n",
    "\t- [RNN Operations](#RNN-Operations)\n",
    "\t- [Forward Pass](#Forward-Pass)\n",
    "\t\t- [Nonlinearity](#Nonlinearity)\n",
    "\t\t- [Full Forward Pass](#Full-Forward-Pass)\n",
    "\t- [Calculating the Loss](#Calculating-the-Loss)\n",
    "\t- [Backward Pass](#Backward-Pass)\n",
    "\t\t- [Full Backward Pass](#Full-Backward-Pass)\n",
    "- [Full Implementation](#Full-Implementation)\n",
    "- [Wrap-up](#Wrap-up)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the [last lesson](../4-Classification/classification.ipynb), we learned how to use a neural network to perform classification.<br>\n",
    "This is one important piece of using a neural network to do NLP (natural language processing).\n",
    "\n",
    "In this lesson, we'll learn another important piece - how to use neural networks to take in sequences of input and make predictions.<br>\n",
    "A sequence of input can be a sentence made up of words, or a series of weather observations.\n",
    "\n",
    "Each element in a sequence has a position.  We can think of those positions as being steps in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a sequence of temperatures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sympy import diff, symbols, exp\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1970-01-01    60.0\n",
      "1970-01-02    52.0\n",
      "1970-01-03    52.0\n",
      "1970-01-04    53.0\n",
      "1970-01-05    52.0\n",
      "1970-01-06    50.0\n",
      "1970-01-07    52.0\n",
      "1970-01-08    56.0\n",
      "1970-01-09    54.0\n",
      "1970-01-10    57.0\n",
      "Name: tmax, dtype: float64 \n",
      "\n",
      "Dataset Size: 13509\n"
     ]
    }
   ],
   "source": [
    "# Read in weather data, fill missing values\n",
    "data = pd.read_csv(\"../data/clean_weather.csv\", index_col=0)\n",
    "data = data.ffill()\n",
    "\n",
    "# First 10 data entries\n",
    "print(data[\"tmax\"].head(10), \"\\n\")\n",
    "print('Dataset Size:', len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first sequence element (at time step $0$) is $60$.<br>\n",
    "The second sequence element (at time step $1$) is $52$, and so on.<br>\n",
    "In total, there are $13509$ timesteps in this sequence dataset, all **sequentially related**.\n",
    "\n",
    "Let's say we want to predict a *next element* within a sequence. Specifically, for our above sequence,<br>\n",
    "we want to predict the value of time step $10$ from the preceeding steps.\n",
    "\n",
    "With a standard dense neural network, you would have to treat each input feature *seperately*.<br>\n",
    "Therefore, it becomes a question of what exactly to input into the network.<br>\n",
    "One time step? Two time steps? The entire sequence?\n",
    "\n",
    "Let's follow the path of using a dense neural network here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60. 52. 52. 53. 52. 50. 52. 56. 54. 57.]]\n"
     ]
    }
   ],
   "source": [
    "# Turn our sequence into a single row of data\n",
    "print(data[\"tmax\"].head(10).to_numpy()[np.newaxis,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As stated prior, we could for example feed the entire sequence as a single training example into a neural network to predict the next temperature.<br>\n",
    "**But what if we don't know the length of the sequence beforehand?**<br>\n",
    "For example, what if we wanted the neural network to predict the next word that came after a variable-length prompt?\n",
    "\n",
    "We could have prompts like:\n",
    "- `Write me a song`\n",
    "- `Tell me a story about dinosaurs`\n",
    "- `Add 1 and 2`\n",
    "\n",
    "We want our network to be able to handle these different inputs robustly.<br>\n",
    "One way to do this is to add extra zeros to the end of the sequence to make all of the sequences the same, biggest, length.<br>\n",
    "Then our network will handle a standard input size.\n",
    "\n",
    "However, this introduces the problem that the neural network will have to learn parameters for each position *separately*, *independently*,<br>\n",
    "without oversight of contextual development of inputs over time.\n",
    "\n",
    "If we're passing in two weather observations of different lengths, the network will have to learn to ignore the zeros at the end of the shorter sequence, and the interplay within the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[60. 52. 52. 53. 52. 50.]]\n",
      "[[64. 63. 62. 61. 60. 62. 67. 66. 70. 62.]]\n"
     ]
    }
   ],
   "source": [
    "print(data[\"tmax\"].head(6).to_numpy()[np.newaxis,:])\n",
    "print(data[\"tmax\"].tail(10).to_numpy()[np.newaxis,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The thing is, the temperatures are interrelated - the temperature today causes a rational basis for the temperature tomorrow.<br>\n",
    "If we train a neural network to evaluate each position within the sequence input separately, which is what a dense neural network does,<br>\n",
    "it won't be able to efficiently learn the time-directional relationship between one sequence position and the next.\n",
    "\n",
    "Instead of the network being able to derive a rule to the extend of \"all time steps are related to their previous time step\",<br>\n",
    "it will be architectually restrained to stating \"time step $0$ is related to time step $1$, time step $1$ is related to time step $2$\", and so on.<br>\n",
    "\n",
    "> Dense Networks, applied to sequence data, lacks efficiency, as it structurally is inapt<br>\n",
    "> to learn the time-directional relationship between sequence elements.\n",
    "\n",
    "Recurrent Neural Networks (RNN) are a special kind of neural networks that solve this problem.\n",
    "\n",
    "In this lesson, we'll learn how to build one in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The RNN Architecture\n",
    "\n",
    "RNNs work by sharing parameters (weights and biases) across steps in a sequence.<br>\n",
    "At a *very* high level, a recurrent neural network looks like this:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/rnn/rnn_rolled.svg\" alt=\"Rolled RNN\">\n",
    "</div>\n",
    "\n",
    "What we see above is called a compact representation of an RNN. We pass input into an RNN, a single entry from a sequence, with the entry being of index $64$.<br>\n",
    "Again, per timestep, one input is made, consisting of one sequence entry, not the entire sequence or anything like that.<br>\n",
    "Each input is passed through the `Input Step (I)`, the `Hidden Step (H)` and the `Output Step (O)`,<br>\n",
    "the latter of which ultimately returning a prediction for the respective next sequence entry.\n",
    "\n",
    "The arrow connecting `H` to itself is called **Recurrence**.\n",
    "\n",
    "> In neural networks, recurrence means that the `H` layers applied per input are connected to each other across time/sequence steps.\n",
    "\n",
    "The effect of Recurrence unravels itself when looking at the unrolled depiction of an RNN:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/rnn/rnn_unrolled.svg\" alt=\"Unrolled RNN\">\n",
    "</div>\n",
    "\n",
    "Each sequence element is labeled at the bottom with the time step of the element.<br>\n",
    "The first element in the sequence is $t_0$, the second is $t_1$, and so on.<br>\n",
    "You can see that each element is passed into the input step, then to the hidden step.<br>\n",
    "The trick is that **the output of the hidden step is passed both to the output, and to the next hidden step**.\n",
    "\n",
    "Therefore, at each step, an RNN is informed about inputs at previous steps.<br>\n",
    "The \"memory\" of the network is stored in the hidden step `H`, and represented as an internal matrix of values.<br>\n",
    "This matrix is updated at each time step with new information about the sequence.\n",
    "\n",
    "Let's actually look at the exact operations that happen per each step within an RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Operations\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/rnn/rnn_operations.svg\" alt=\"RNN Operations\">\n",
    "</div>\n",
    "\n",
    "**An RNN has $3$ steps:**\n",
    "- `Input`  - Takes in a *sequence item*, multiplies it by some *input weight*,\n",
    "- `Hidden` - Takes the *previous hidden state*, multiplies it by some *hidden weight*. The *input state* is added to that, then a nonlinear activation function is applied.\n",
    "- `Output` - Takes the *hidden state*, multiplies it by some *output weight*.\n",
    "\n",
    "Additionally, biases are added at the *hidden step* and *output step*, but the biases are kept out of the diagram to make it easier to read.\n",
    "\n",
    "The trickiest architectural feat of an RNN is the hidden step.<br>\n",
    "It's this step that gives an RNN its power, by enabling it to have \"memory\".<br>\n",
    "The hidden state of the RNN stores a representation of information aggregated across previous sequence elements.<br>\n",
    "The hidden weights enable the RNN to go on and selectively remember/imprint or forget/erase certain information about past sequence elements.\n",
    "\n",
    "This way, the RNN can have trainable/focusable knowledge of past elements in the sequence without having strictly separate parameters for each sequence item.<br>\n",
    "We instead reuse them, inform our current step with them, and update them with new information, as seen fit.<br>\n",
    "The overall process is called **parameter sharing** - the RNN step-by-step optimizes the *same* set of input, hidden, and output weights through every sequence element.<br>\n",
    "Therefore, input, hidden and output are imprints of sequence characteristics, not of individual sequence elements. This is efficient, compared to dense neural networks.\n",
    "\n",
    "In addition to the sequence-focus, *parameter sharing* enables the RNN to be used for sequences of variable length - **we don't require a fixed size input sequence length**.\n",
    "\n",
    "### Forward Pass\n",
    "\n",
    "Let's go through an example to see how RNNs are implemented.<br>\n",
    "We'll initialize each weight matrix, and then do a sample forward pass with a sequence of length $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66. 70. 62.]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(0) # Seed for reproducibility\n",
    "\n",
    "i_weight = np.random.rand(1,2) # Input weight,  (1,2) matrix\n",
    "h_weight = np.random.rand(2,2) # Hidden weight, (2,2) matrix\n",
    "o_weight = np.random.rand(2,1) # Output weight, (2,1) matrix\n",
    "\n",
    "# Assemble a sequence of 3 temperature values from dataset\n",
    "temps = data[\"tmax\"].tail(3).to_numpy()\n",
    "print(temps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The network will take in a single input feature, turn it into $2$ representing hidden features, and then turn that into a single output feature.<br>\n",
    "We can now format our input for the forward pass.<br>\n",
    "Specifically, we will reshape each input scalar into a $1\\times 1$ matrix to make multiplication more straightforward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape each scalar to a (1,1) matrix, \n",
    "# Assign reshaped sequence input at each time step to respective variable.\n",
    "x0 = temps[0].reshape(1,1)\n",
    "x1 = temps[1].reshape(1,1)\n",
    "x2 = temps[2].reshape(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can go on and feed the element at time step $0$ into our network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input result at t=0:\t[[36.22169126 47.20249818]]\n",
      "Hidden state at t=0:\t[[36.22169126 47.20249818]] \n",
      "\n",
      "RNN Output - Step 0:\t[[57.94406231]]\n",
      "Actual next temperature:\t[[70.]]\n"
     ]
    }
   ],
   "source": [
    "# Multiply input by input weight to get input result for time step 0\n",
    "xi_0 = x0 @ i_weight\n",
    "\n",
    "# There is no previous time step, so there is no hidden state\n",
    "# Apply ReLU to input result, makes for initial hidden state\n",
    "xh_0 = np.maximum(0, xi_0)\n",
    "\n",
    "# Get output result at time step 0\n",
    "xo_0 = xh_0 @ o_weight\n",
    "\n",
    "print(f'Input result at t=0:\\t{xi_0}')\n",
    "print(f'Hidden state at t=0:\\t{xh_0}', \"\\n\")\n",
    "print(f'RNN Output - Step 0:\\t{xo_0}')\n",
    "print(f'Actual next temperature:\\t{x1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xo_0` is the RNN's prediction for the next step in the sequence.<br>\n",
    "With `xo_0` derived, we can move the network forward to time step $1$.<br>\n",
    "Notice how we didn't yet do anything to optimize the weights. This will happen later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Output - Step 1: [[124.54916092]]\n"
     ]
    }
   ],
   "source": [
    "# We feed the input in the same way as the previous time step\n",
    "xi_1 = x1 @ i_weight\n",
    "\n",
    "# This time, we do have a previous time step, so we calculate xh\n",
    "# This is multiplying the previous hidden state xh_0 by the hidden weights\n",
    "xh = xh_0 @ h_weight\n",
    "\n",
    "# We add the previous hidden state (times h_weight) to the input at time step 1\n",
    "xh_1 = np.maximum(0, xh + xi_1)\n",
    "\n",
    "# We again find the output by multiplying xh_1 by the output weight\n",
    "xo_1 = xh_1 @ o_weight\n",
    "\n",
    "print('RNN Output - Step 1:', xo_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`xo_1` is our prediction for the next sequence element.<br>\n",
    "Now we can do the same for our final time step $2$.<br>\n",
    "Notice how we use `i_weight`, `h_weight`, and `o_weight` again for this time step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN Output - Step 2: [[190.94853131]]\n"
     ]
    }
   ],
   "source": [
    "# We feed the input in the same way as the previous time step\n",
    "xi_2 = x2 @ i_weight\n",
    "\n",
    "# This time, we do have a previous time step, so we calculate xh\n",
    "# This is multiplying the previous hidden state xh_1 by the hidden weights\n",
    "xh = xh_1 @ h_weight\n",
    "\n",
    "# We add the previous hidden state (times h_weight) to the input at time step 2\n",
    "xh_2 = np.maximum(0, xh + xi_2)\n",
    "\n",
    "# We again find the output by multiplying xh_1 by the output weight\n",
    "xo_2 = xh_2 @ o_weight\n",
    "\n",
    "print('RNN Output - Step 2:', xo_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now passed through $3$ forward steps of our RNN!<br>\n",
    "The output `x0` at each time step $i$ is the prediction for the respective next element $i+1$ in the sequence.<br>\n",
    "The hidden state of the RNN allows the network to utilize information from all prior sequence elements.<br>\n",
    "Therefore, when we're processing the sequence item at time step $2$, the hidden state of the RNN aggregates information about the sequence elements at time steps $0$ and $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nonlinearity\n",
    "\n",
    "You may have noticed that the output values increased in each time step of our RNN.<br>\n",
    "That happened because the hidden state kept increasing in size.<br>\n",
    "Unlike the *Sigmoid* or *Softmax* activation functions, *ReLU* doesn't change the scale of the positive inputs at all.<br>\n",
    "This means that some positive values get repeatedly multiplied and therefore grow larger and larger.\n",
    "\n",
    "We can see this below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden State - Step 0: [[36.22169126 47.20249818]]\tOutput: [[57.94406231]]\t\tActual: [[66.]]\n",
      "Hidden State - Step 1: [[ 80.24761908 100.28766177]]\tOutput: [[124.54916092]]\tActual: [[70.]]\n",
      "Hidden State - Step 2: [[124.88411227 152.84252918]]\tOutput: [[190.94853131]]\tActual: [[62.]]\n"
     ]
    }
   ],
   "source": [
    "print(f'Hidden State - Step 0: {xh_0}\\tOutput: {xo_0}\\t\\tActual: {x0}')\n",
    "print(f'Hidden State - Step 1: {xh_1}\\tOutput: {xo_1}\\tActual: {x1}')\n",
    "print(f'Hidden State - Step 2: {xh_2}\\tOutput: {xo_2}\\tActual: {x2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our hidden state at time step $2$ is much larger than our initial hidden state.<br>\n",
    "To fix this, we usually replace our ReLU with the $\\tanh$ activation function for RNNs.\n",
    "\n",
    "The equation for $\\tanh$ is this:\n",
    "$$\\tanh = \\dfrac{e^{x}-e^{-x}}{e^{x}+e^{-x}}$$\n",
    "\n",
    "As $x$ gets large, $e^{x}$ gets very large, and $e^{-x}$ gets very small. This pushes the output of $\\tanh$ towards $1$.<br>\n",
    "Then in turn, when $x$ gets very small (negative), $e^{-x}$ gets very large, and $e^{x}$ gets very small. This results in trending towards $-1$.\n",
    "\n",
    "Let's graph the $\\tanh$ function to see how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define some temperature values\n",
    "temps = np.arange(-10, 10, .1)\n",
    "\n",
    "plt.title('Activation Functions')\n",
    "plt.plot(temps, np.tanh(temps), label='Tanh')\n",
    "plt.plot(temps, np.maximum(0, temps), label='ReLU')\n",
    "plt.plot(temps, 1/(1+np.exp(-temps)), label='Sigmoid')\n",
    "plt.ylim(-1.5, 1.5)\n",
    "plt.xlim(-10, 10)\n",
    "plt.axhline(0, color='black', lw=.5)\n",
    "plt.axvline(0, color='black', lw=.5)\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\tanh$ outputs values between $-1$ and $1$. Additionally, it has a very steep slope around $0$ on the x-axis.<br>\n",
    "The $\\tanh$ activation function is better for our purposes compared to Sigmoid, because it enables us to have *negative values*.<br>\n",
    "This enables the gradient to be steeper, which aids in gradient descent.<br>\n",
    "You can also view this from a totally different perspective: In enabling negative values,<br>\n",
    "$\\tanh$ allows for the hidden state to learn to forget certain information through scaling down the values, too.\n",
    "\n",
    "Let's take a look at the derivative of $\\tanh$ using sympy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{\\left(- e^{x} + e^{- x}\\right) \\left(e^{x} - e^{- x}\\right)}{\\left(e^{x} + e^{- x}\\right)^{2}} + 1$"
      ],
      "text/plain": [
       "(-exp(x) + exp(-x))*(exp(x) - exp(-x))/(exp(x) + exp(-x))**2 + 1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = symbols(\"x\")\n",
    "\n",
    "# tanh\n",
    "sympy_tanh = (exp(x) - exp(-x)) / (exp(x) + exp(-x))\n",
    "\n",
    "# Derivative of tanh\n",
    "diff(sympy_tanh, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the result is technically correct, Sympy doesn't quite simplify the derivative all the way.<br>\n",
    "Like the derivatives of sigmoid and softmax, we can express the derivative of $\\tanh{x}$ in terms of the output: $1 - \\tanh^2{x}$.\n",
    "\n",
    "We can then graph this derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title('Derivative of Tanh')\n",
    "plt.plot(temps, 1 - np.tanh(temps) ** 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the derivative has a steep slope, making $\\tanh$ an ideal activation function.<br>\n",
    "We can say that because this slope will enable gradient descent to descend effectively and quickly.<br>\n",
    "We can now go on with our own implementation and use $\\tanh$ instead of ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Forward Pass\n",
    "\n",
    "We now know enough to put together a *complete* RNN forward pass.<br>\n",
    "We'll first initialize our weights and biases. We'll add in bias terms in the hidden and output steps.\n",
    "\n",
    "We'll also scale the weights and biases to work properly with the $\\tanh$ nonlinearity.<br>\n",
    "We'll make our input and hidden weights small, so that $\\tanh$ doesn't squash all the values to its extremes $1$ or $-1$.<br>\n",
    "We'll make the output weight large, since the output of the hidden step will be small (between $1$ and $-1$), yet we actually want to cover some output value range.\n",
    "\n",
    "Of course, in a comprehensive RNN implementation, the network would eventually learn the correct parameters.<br>\n",
    "But initializing weights and biases to the correct ranges helps *a lot* with gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0) # Reproducibility\n",
    "\n",
    "# Define weights and biases\n",
    "# Scale w and b so tanh activations don't radicalize too much\n",
    "i_weight = np.random.rand(1,5) / 5 - .1 # we sample values [-0.1;0.1]\n",
    "h_weight = np.random.rand(5,5) / 5 - .1\n",
    "h_bias   = np.random.rand(1,5) / 5 - .1\n",
    "\n",
    "# Tanh pushes values to between -1 and 1, so scale up the output weights\n",
    "# We sample large output values, allowing for displaying larger ranges despite tanh input\n",
    "o_weight = np.random.rand(5,1) * 50\n",
    "o_bias   = np.random.rand(1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now we can write the forward pass as a for loop.<br>\n",
    "The loop will process sequence elements one after another.\n",
    "\n",
    "We'll store the output prediction and the hidden state, respectively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Forward Pass for sequences of 3 time steps\n",
    "seq_length = 3\n",
    "\n",
    "outputs = np.zeros(seq_length)   # Array to store the output predictions\n",
    "hiddens = np.zeros((seq_length, 5)) # Array to store hidden states for use in backprop\n",
    "\n",
    "# This will store the previous hidden state, \n",
    "# We'll need prev_hidden to calculate the current hidden step\n",
    "prev_hidden = None\n",
    "\n",
    "# Our temperature sequence to train on\n",
    "sequence = data[\"tmax\"].tail(seq_length).to_numpy()\n",
    "\n",
    "for i in range(len(sequence)):\n",
    "    # Get scalar input from sequence at current position\n",
    "    x = sequence[i].reshape(1,1)\n",
    "\n",
    "    # Multiply input by input weight\n",
    "    xi = x @ i_weight\n",
    "\n",
    "    if prev_hidden is None:\n",
    "        # Init hidden as input\n",
    "        xh = xi\n",
    "    else:\n",
    "        # Add prev hidden to input + bias\n",
    "        xh = xi + (prev_hidden @ h_weight) + h_bias\n",
    "\n",
    "    # Apply tanh to hidden state\n",
    "    xh = np.tanh(xh)\n",
    "    # Store hidden state for next iteration\n",
    "    prev_hidden = xh\n",
    "    # Store hidden state for backprop\n",
    "    hiddens[i,] = xh\n",
    "\n",
    "    # Multiply hidden state by output weight, add bias\n",
    "    xo = xh @ o_weight + o_bias\n",
    "    # Store output\n",
    "    outputs[i] = xo[0,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Afterwards, we can take a look at our outputs and hidden states.<br>\n",
    "As we can see, the hidden states don't increase constantly anymore:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[74.31470595 80.66149404 77.67852446] \n",
      "\n",
      "[[ 0.56784618  0.99320288  0.87557333  0.53166114 -0.76483255]\n",
      " [ 0.58366756  0.99568651  0.90034879  0.69338529 -0.84149203]\n",
      " [ 0.5383306   0.99164251  0.86287584  0.66091071 -0.80543591]]\n"
     ]
    }
   ],
   "source": [
    "print(outputs, \"\\n\")\n",
    "print(hiddens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Calculating the Loss\n",
    "\n",
    "After we progressed through our forward pass, we can head on to calculating the gradient with respect to the network outputs.<br>\n",
    "We will be determining the mean squared error `mse`. From there, we'll also calculate `mse_grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(actual, predicted):\n",
    "    return np.mean((actual-predicted)**2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return (predicted - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4.31470595 18.66149404 13.67852446]\n"
     ]
    }
   ],
   "source": [
    "# Actual next day temperatures\n",
    "actuals = data[\"tmax_tomorrow\"].tail(seq_length).to_numpy()\n",
    "\n",
    "loss_grad = mse_grad(actuals, outputs)\n",
    "print(loss_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Backward Pass\n",
    "\n",
    "Now that we've written a forward pass, let's think about the backward pass to update our model parameters.<br>\n",
    "The main complexity in the backward pass is that parameters impact both the current and future outputs.\n",
    "\n",
    "Let's visualize this, starting with the last sequence item:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/rnn/gradient_last.svg\" alt=\"Gradient Last\">\n",
    "</div>\n",
    "\n",
    "As you can see above, the last hidden step is only used by the output of the last time step.<br>\n",
    "But this is different for other hidden steps:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/rnn/gradient_inside.svg\" alt=\"Gradient Inside\">\n",
    "</div>\n",
    "\n",
    "The hidden step at time step $2$ is connected to both the output and the next hidden state.<br>\n",
    "So it affects not just the current output, but all subsequent outputs, too.\n",
    "\n",
    "We have to consider this fact when we do backpropagation: **Some parameters will impact multiple outputs**.<br>\n",
    "These parameters need to get gradients assembled from multiple outputs in order to be correctly adjusted.\n",
    "\n",
    "> We'll have to send the gradient with respect to each hidden step backwards to the previous sequence position.<br>\n",
    "> This is called backpropagation through time, and it's how we train the parameters of an RNN.\n",
    "\n",
    "Here are the operations we will follow to do a backward pass:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/rnn/rnn_operations_bw.svg\" alt=\"RNN Operations\">\n",
    "</div>\n",
    "\n",
    "Let's implement the backward pass one by one for each sequence element.\n",
    "\n",
    "We start out by creating variables to store the gradients with respect to each parameter.<br>\n",
    "We do this because we want to sum the gradients across time positions before making updates with gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregators, keeping running total of gradients\n",
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Then, we can run backpropagation for the last sequence element.<br>\n",
    "In this case, there is no next hidden state, so we only need to worry about the gradient wrt the output at this position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get the loss wrt the output at the current time step\n",
    "l2_grad = loss_grad[2].reshape(1,1)\n",
    "\n",
    "# Add to the output weight gradient\n",
    "# Multiply the output of the hidden step (hiddens[2]) transposed by the l2 grad\n",
    "# np.newaxis creates a new size 1 axis, effectively transposing the hiddens\n",
    "o_weight_grad += hiddens[2][:,np.newaxis] @ l2_grad\n",
    "# Add to the bias gradient.  Similar to a dense neural network, this is just the mean of the l2_grad.\n",
    "o_bias_grad += np.mean(l2_grad)\n",
    "\n",
    "# Find the gradient wrt the hidden step output\n",
    "h2_grad = l2_grad @ o_weight.T\n",
    "\n",
    "# Derivative of the tanh function\n",
    "tanh_deriv = 1 - hiddens[2,:][np.newaxis,:] ** 2\n",
    "# Multiply each position in the h_grad by the tanh derivative - this \"undoes\" the tanh in the forward pass\n",
    "h2_grad = np.multiply(h2_grad, tanh_deriv)\n",
    "\n",
    "# Now, find how much we need to update the hidden weights.\n",
    "# We take the input to the hidden step (the output of the previous hidden step in the forward pass) @ h2_grad\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h2_grad\n",
    "h_bias_grad += np.mean(h2_grad)\n",
    "\n",
    "# This multiples the sequence value at time step 2 by the gradient\n",
    "# We don't need the .T here, but I left it here in case you have a larger input size\n",
    "i_weight_grad += sequence[2].reshape(1,1).T @ h2_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "A lot of the above step is very similar to backpropagation in a [dense neural network](../3-Dense_Networks/dense.ipynb).<br>\n",
    "The main difference comes in the next sequence position, where we need to consider multiple gradients at the hidden step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l1_grad = loss_grad[1].reshape(1,1)\n",
    "\n",
    "o_weight_grad += hiddens[1][:,np.newaxis] @ l1_grad\n",
    "o_bias_grad += np.mean(l1_grad)\n",
    "\n",
    "h1_grad = l1_grad @ o_weight.T\n",
    "\n",
    "# We do have a next sequence position (2), so we need to include that gradient\n",
    "# We multiply the h2 gradient by the weight to pull it back to the current sequence position\n",
    "h1_grad += h2_grad @ h_weight.T\n",
    "\n",
    "# The rest of the operation is the same\n",
    "tanh_deriv = 1 - hiddens[1,:][np.newaxis,:] ** 2\n",
    "h1_grad = np.multiply(h1_grad, tanh_deriv)\n",
    "\n",
    "h_weight_grad += hiddens[1,:][:,np.newaxis] @ h1_grad\n",
    "h_bias_grad += np.mean(h1_grad)\n",
    "\n",
    "i_weight_grad += sequence[1].reshape(1,1).T @ h1_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now, we can do the final sequence position, $0$.<br>\n",
    "The main difference here is that we don't update the hidden gradient, since<br>\n",
    "there is no previous sequence position that gave us hidden state input in the forward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "l0_grad = loss_grad[0].reshape(1,1)\n",
    "\n",
    "o_weight_grad += hiddens[0][:,np.newaxis] @ l0_grad\n",
    "o_bias_grad += np.mean(l0_grad)\n",
    "\n",
    "h0_grad = l0_grad @ o_weight.T\n",
    "h0_grad += h1_grad @ h_weight.T\n",
    "\n",
    "tanh_deriv = 1 - hiddens[0,:][np.newaxis,:] ** 2\n",
    "h0_grad = np.multiply(h0_grad, tanh_deriv)\n",
    "\n",
    "# We don't update the hidden weight, since there was no previous hidden state\n",
    "# We can update the hidden bias if you want\n",
    "\n",
    "i_weight_grad += sequence[0].reshape(1,1).T @ h0_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now look at our gradient updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Weight Gradient: [[ 20.7057662 ]\n",
      " [ 36.43058259]\n",
      " [ 32.38256333]\n",
      " [ 24.27385028]\n",
      " [-30.02070081]] \n",
      "\n",
      "Output Bias Gradient: 36.65472444998886 \n",
      "\n",
      "Hidden Weight Gradient: [[ 414.96700557    7.11705057  128.09069049  490.00406461  206.76584453]\n",
      " [ 707.89791607   12.14107435  218.51166876  835.90466603  352.72469496]\n",
      " [ 640.11616967   10.97855755  197.58901567  755.86617912  318.95104586]\n",
      " [ 492.97243446    8.45491256  152.16915725  582.11494741  245.6336537 ]\n",
      " [-598.27109131  -10.2608775  -184.67241047 -706.45439889 -298.10087499]] \n",
      "\n",
      "Hidden Bias Gradient: 427.27906887063534 \n",
      "\n",
      "Input Weight Gradient: [[56026.07073088   960.33350651 16758.78602233 67910.52141452\n",
      "  28061.23683219]]\n"
     ]
    }
   ],
   "source": [
    "print('Output Weight Gradient:', o_weight_grad, \"\\n\")\n",
    "print('Output Bias Gradient:', o_bias_grad, \"\\n\")\n",
    "print('Hidden Weight Gradient:', h_weight_grad, \"\\n\")\n",
    "print('Hidden Bias Gradient:', h_bias_grad, \"\\n\")\n",
    "print('Input Weight Gradient:', i_weight_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We've now completed backpropagation across $3$ time steps!<br>\n",
    "We'll see how to make the gradient updates in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Full Backward Pass\n",
    "\n",
    "Similar to a forward pass, we can implement the full backward pass as a loop.<br>Here we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hidden = None\n",
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad = [0] * 5\n",
    "loss_grad = mse_grad(actuals, outputs) # Gradient of mse across all sequence steps, yields (seq_length,1) matrix\n",
    "\n",
    "# Iterating from 2 to 0 (inclusive of 0)\n",
    "for i in range(2, -1, -1):\n",
    "    # Plug out current loss gradient scalar into (1,1) matrix\n",
    "    l_grad = loss_grad[i].reshape(1,1)\n",
    "    # Add to the output weight gradient\n",
    "    # Multiply the output of the hidden step transposed by the l_grad\n",
    "    # np.newaxis creates new size 1 axis, effectively transposing the hiddens\n",
    "    o_weight_grad += hiddens[i][:,np.newaxis] @ l_grad\n",
    "    # The bias grad is just the mean of the l_grad, like in DNNs.\n",
    "    o_bias_grad += np.mean(l_grad)\n",
    "    # Gradient of output layer w.r.t the loss\n",
    "    o_grad = l_grad @ o_weight.T\n",
    "\n",
    "    # Gradient calculation is different, given a next hidden state\n",
    "    if next_hidden is None:\n",
    "        # We're at the end of a sequence\n",
    "        h_grad = o_grad\n",
    "    else:\n",
    "        # We're at the start or within a sequence\n",
    "        h_grad = o_grad + next_hidden @ h_weight.T\n",
    "\n",
    "    # Derivative of tanh being applied to hidden state\n",
    "    tanh_deriv = 1 - hiddens[i,:][np.newaxis,:] ** 2\n",
    "    # The actual h_grad is bare gradient affected/multiplied by activation function derivative\n",
    "    h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "    # Store current hidden state for next/earlier cell gradient calculation\n",
    "    next_hidden = h_grad\n",
    "\n",
    "    # Don't update hidden weights for initial (chronologically first) sequence position\n",
    "    if i > 0:\n",
    "        h_weight_grad += hiddens[i-1,:][:,np.newaxis] @ h_grad\n",
    "        h_bias_grad += np.mean(h_grad)\n",
    "\n",
    "    # Ultimately, update input weight gradient\n",
    "    i_weight_grad += sequence[i].reshape(1,1).T @ h_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use gradient descent to make parameter updates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-6 # We'll just use this fixed learning rate for now\n",
    "\n",
    "# Dividing the learning rate by the sequence length applies the learning rate across the update,\n",
    "# not per time step. While this may seem counterintuitive, this helps stabilize training.\n",
    "lr = lr / seq_length\n",
    "\n",
    "# Gradient Descent Update\n",
    "i_weight -= i_weight_grad * lr\n",
    "h_weight -= h_weight_grad * lr\n",
    "h_bias -= h_bias_grad * lr\n",
    "o_weight -= o_weight_grad * lr\n",
    "o_bias -= o_bias_grad * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Full Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now go on to do a full implementation of a complete network for a sequence length of $3$ and a time step input's dimensionality of $1$.<br>\n",
    "For the full implementation, we'll use $3$-dimensional inputs and a sequence length of $7$.<br>\n",
    "This code will mostly be the same as the forward and backward passes we already implemented, just a bit more condensed.\n",
    "\n",
    "First, we'll load in and scale our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define predictor fields to consider, target to predict for\n",
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "# Reproducibly split into train, valid, test set\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]\n",
    "\n",
    "# Standardize each dataset individually to avoid data leakage\n",
    "scaler = StandardScaler()\n",
    "train_x = scaler.fit_transform(train_x)\n",
    "valid_x = scaler.fit_transform(valid_x)\n",
    "test_x = scaler.fit_transform(test_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can now initialize our weights and biases.<br>\n",
    "We'll scale our parameters so they start out relatively small.<br>\n",
    "\n",
    "This helps the network converge meaningfully, because $\\tanh$ will meaningfully scale the values not just to extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        np.random.seed(0)\n",
    "        k = 1/math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        \n",
    "        # Weights for input layer\n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        \n",
    "        # Weights + Bias for hidden layer\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        \n",
    "        # Weights + Bias for output layer\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        \n",
    "        # Append the weights and biases to the layers list\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The forward pass now looks something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(x, layers):\n",
    "    hiddens = []\n",
    "    outputs = []\n",
    "    for i in range(len(layers)):\n",
    "        # Unpack weights and biases\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        \n",
    "        # Shaping Hidden state (seq_len, hidden_units) and Output (seq_len, output_units)\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1]))\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1]))\n",
    "        \n",
    "        # Iterate over each time step\n",
    "        for j in range(x.shape[0]):\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias\n",
    "            \n",
    "            # Activation. tanh squashes values between -1 and 1; \n",
    "            # Allows for incrementing/decrementing scales to retain/forget\n",
    "            hidden_x = np.tanh(hidden_x)\n",
    "            \n",
    "            # Store hidden for use in backprop\n",
    "            hidden[j,:] = hidden_x\n",
    "            \n",
    "            # Output layer\n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            \n",
    "            # Store output for use in backprop\n",
    "            output[j,:] = output_x\n",
    "       \n",
    "        # Store hidden state and output for use in backprop\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "   \n",
    "    # We want all hidden states and only the 'next in line' prediction\n",
    "    return hiddens, outputs[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "And the backward pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]\n",
    "        hidden = hiddens[i]\n",
    "        next_h_grad = None\n",
    "        i_weight_grad, h_weight_grad, h_bias_grad, o_weight_grad, o_bias_grad = [0] * 5\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):\n",
    "            # Add newaxis in the first dimension\n",
    "            out_grad = grad[j,:][np.newaxis, :]\n",
    "\n",
    "            # Output updates\n",
    "            # np.newaxis creates a size 1 axis, in this case transposing matrix\n",
    "            o_weight_grad += hidden[j,:][:, np.newaxis] @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate gradient to hidden unit\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Then we multiply the gradient by the hidden weights to pull gradient from next hidden state to current hidden state\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                # Add the gradients together to combine output contribution and hidden contribution\n",
    "                h_grad += hh_grad\n",
    "\n",
    "            # Pull the gradient across the current hidden nonlinearity\n",
    "            # derivative of tanh is 1 - tanh(x) ** 2\n",
    "            # So we take the output of tanh (next hidden state), and plug in\n",
    "            tanh_deriv = 1 - hidden[j][np.newaxis,:] ** 2\n",
    "\n",
    "            # next_h_grad @ np.diag(tanh_deriv_next) multiplies each element of next_h_grad by the deriv\n",
    "            # Effect is to pull value across nonlinearity\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            # Store to compute h grad for previous sequence position\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            # If we're not at the very beginning\n",
    "            if j > 0:\n",
    "                # Multiply input from previous layer by post-nonlinearity grad at current layer\n",
    "                h_weight_grad += hidden[j-1][:, np.newaxis] @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            i_weight_grad += x[j,:][:,np.newaxis] @ h_grad\n",
    "\n",
    "        # Normalize lr by number of sequence elements\n",
    "        lr = lr / x.shape[0]\n",
    "        i_weight -= i_weight_grad * lr\n",
    "        h_weight -= h_weight_grad * lr\n",
    "        h_bias -= h_bias_grad * lr\n",
    "        o_weight -= o_weight_grad * lr\n",
    "        o_bias -= o_bias_grad * lr\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can conclude by setting up a training loop and measuring error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\t Train Loss: 3127.0305785639307\t Val Loss: 2173.940590560558\n",
      "Epoch: 50\t Train Loss: 29.69054028769555\t Val Loss: 34.48611521186155\n",
      "Epoch: 100\t Train Loss: 25.06454162741681\t Val Loss: 27.155395197347143\n",
      "Epoch: 150\t Train Loss: 22.889655164162015\t Val Loss: 23.846369196866874\n",
      "Epoch: 200\t Train Loss: 22.235640463748837\t Val Loss: 22.853770518905\n",
      "Epoch: 249\t Train Loss: 22.08072534106742\t Val Loss: 22.54001365658709\n"
     ]
    }
   ],
   "source": [
    "epochs = 250\n",
    "lr = 1e-5\n",
    "sequence_len = 7\n",
    "\n",
    "# Per time step: Three inputs, Four hidden units, One output\n",
    "layer_conf = [\n",
    "    {\"type\":\"input\", \"units\": 3},\n",
    "    {\"type\": \"rnn\", \"hidden\": 4, \"output\": 1}\n",
    "]\n",
    "\n",
    "# Initialize weights and biases\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    epoch_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j:(j+sequence_len),]\n",
    "        seq_y = train_y[j:(j+sequence_len),]\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = mse_grad(seq_y, outputs)\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs)\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == epochs - 1:\n",
    "        valid_loss = 0\n",
    "        for j in range(valid_x.shape[0] - sequence_len):\n",
    "            seq_x = valid_x[j:(j+sequence_len),]\n",
    "            seq_y = valid_y[j:(j+sequence_len),]\n",
    "            _, outputs = forward(seq_x, layers)\n",
    "            valid_loss += mse(seq_y, outputs)\n",
    "\n",
    "        print(f\"Epoch: {epoch+1}\\t Train Loss: {epoch_loss / len(train_x)}\\t Val Loss: {valid_loss / len(valid_x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "The network reduces training *and* validation loss over successive epochs.<br>\n",
    "Our literal implementation will perform similarly to the RNN implementation in Pytorch, but it will descend *way* more slowly.\n",
    "\n",
    "Anyway, let's see how our own, trained RNN performs on a test sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 64.20\tActual: 69.0\n",
      "Predicted: 59.75\tActual: 67.0\n",
      "Predicted: 70.65\tActual: 70.0\n",
      "Predicted: 61.25\tActual: 61.0\n",
      "Predicted: 69.33\tActual: 75.0\n",
      "Predicted: 63.64\tActual: 71.0\n",
      "Predicted: 81.96\tActual: 68.0\n",
      "Predicted: 66.84\tActual: 82.0\n",
      "Predicted: 62.35\tActual: 68.0\n",
      "Predicted: 62.57\tActual: 60.0\n",
      "Test loss: 38.4535\n"
     ]
    }
   ],
   "source": [
    "# Test the model on test_x, test_y\n",
    "test_loss = 0\n",
    "for j in range(test_x.shape[0] - sequence_len):\n",
    "    seq_x = test_x[j:(j+sequence_len),]\n",
    "    seq_y = test_y[j:(j+sequence_len),]\n",
    "    _, outputs = forward(seq_x, layers)\n",
    "    test_loss += mse(seq_y, outputs)\n",
    "    if j % int(test_x.shape[0] / 10) == 0:\n",
    "        print(f\"Predicted: {outputs[-1][0]:.2f}\\tActual: {seq_y[-1,0]}\")\n",
    "\n",
    "print(f\"Test loss: {test_loss / len(test_x):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap-up\n",
    "\n",
    "We learned a lot in this lesson! You should now have a good idea of how to train a recurrent neural network.<br>\n",
    "If some of the concepts immediately, don't worry about it.<br>\n",
    "RNNs are *tricky*, and take multiple readings and hands-on trials to really understand.\n",
    "\n",
    "In the next lesson, we'll learn about how we can reduce test and validation error with regularization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

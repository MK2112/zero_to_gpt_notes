{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in depth\n",
    "\n",
    "---\n",
    "\n",
    "*You can watch a [video](https://youtu.be/RyKrG8rTGUY) of this lesson if you prefer.*\n",
    "\n",
    "---\n",
    "\n",
    "## What is Backpropagation?\n",
    "\n",
    "At this point, we know how to build neural network architectures using components like [dense layers](../3-Dense_Networks/dense.ipynb),\n",
    "[softmax](../4-Classification/classification.ipynb), and [recurrent layers](../5-RNN/rnn.ipynb).\n",
    "\n",
    "Up until now, we've been loose with covering backpropagation. The neural network architecture itself was more be in focus.<br>\n",
    "Understanding how Backprop works is important for tuning networks for performance, and achieving useful results in the first place.<br>\n",
    "In this lesson, we'll do a deep dive into how backpropagation works.\n",
    "\n",
    "> Backpropagation is how a neural network calculates how much to change each parameter in the network (the gradient).\n",
    "\n",
    "We'll utilize a computational graph to keep track of which changes we make to input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(\"../data\"))\n",
    "from csv_data import SkyServerDatasetWrapper\n",
    "from comp_graph.graph import Node, Parameter, display_chain\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Graphs\n",
    "\n",
    "A computational graph looks like this:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/comp_graph/comp_graph.png\" alt=\"Computational Graph\">\n",
    "</div>\n",
    "\n",
    "In chronological order, this graph shows all the individual operations we performed (mostly multiplication, addition) to modify the value of input $X$.<br>\n",
    "Keeping track of a operations in a computational graph enables us to realiably reverse our operations to do backpropagation.<br>\n",
    "Actually, frameworks like PyTorch apply the same methodology to keep track of interrelations during forward and backward passes.\n",
    "\n",
    "To create the computational graph, we'll make nothing less but a miniature version of PyTorch!\n",
    "\n",
    "## Graphing the Softmax Function\n",
    "\n",
    "Let's first build a computational graph of the softmax function, then backpropagate through that graph to get the gradient for our inputs.<br>\n",
    "We introduced the softmax function in a [previous lesson](../4-Classification/classification.ipynb).<br>\n",
    "Softmax is used to convert the output of a neural network into a probability distribution, e.g. for predictions like classification.\n",
    "\n",
    "To recall, Softmax is defined as:\n",
    "$$\\zeta=\\frac{e^{\\hat{y_{i}}}}{\\sum_{j=0}e^{\\hat{y_{j}}}}$$\n",
    "\n",
    "For each row of our neural network output, we raise $e$ to the power of our output value, then<br>\n",
    "divide by the sum of $e$ raised to the power of each of the outputs for that row.\n",
    "\n",
    "This is what softmax looks like in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_func(normalized):\n",
    "    raised = np.exp(normalized)\n",
    "    output = raised / np.sum(raised, axis=1).reshape(-1,1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test softmax using some fake data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 rows, 3 columns, random numbers\n",
    "x = np.random.rand(5, 3)\n",
    "\n",
    "# Generate random correct labels for later\n",
    "# >Exactly< one label per row is correct\n",
    "y = np.zeros_like(x)\n",
    "y[(np.arange(0,y.shape[0]), np.random.randint(0, 3, size=y.shape[0]))] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix $x$ is our input to the softmax function. It has $3$ columns/classes, $5$ rows/samples.<br>\n",
    "The labels $y$ form our target. Each row is a one-hot encoded vector of $3$ entries each.<br>\n",
    "The $1$ will correspond to the correct label for each row.<br>\n",
    "We set $y$ (and $x$) randomly, but formally correctly, meaning each vector only ever contains one $1$ and two $0$s.\n",
    "\n",
    "We can now go ahead and apply the softmax function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45362078 0.23360927 0.31276996]\n",
      " [0.3695646  0.29438625 0.33604914]\n",
      " [0.21821758 0.33880905 0.44297337]\n",
      " [0.36714924 0.29180974 0.34104102]\n",
      " [0.42520741 0.36985653 0.20493606]] \n",
      "\n",
      "Output-wise summations: [1. 1. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "normalized = x - np.max(x, axis=-1).reshape(-1,1)\n",
    "print(softmax_func(normalized), \"\\n\")\n",
    "print('Output-wise summations:', np.sum(softmax_func(normalized), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We subtract the maximum from each element in the row before passing the data into the softmax function.<br>\n",
    "This prevents numerical overflow or underflow, because each [numeric type](https://numpy.org/doc/stable/user/basics.types.html) (float, integer, etc) can only hold a certain number of digits.<br>\n",
    "When we raise $e$ to a very large or small number, we can generate a number that is too large to store in our specific data type.<br>\n",
    "**Subtracting the max gives us the same end result, but reduces the risk of overflow or underflow.**\n",
    "\n",
    "Feel free to try out softmax with and without subtracting the max to see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45362078 0.23360927 0.31276996]\n",
      " [0.3695646  0.29438625 0.33604914]\n",
      " [0.21821758 0.33880905 0.44297337]\n",
      " [0.36714924 0.29180974 0.34104102]\n",
      " [0.42520741 0.36985653 0.20493606]] \n",
      "\n",
      "[[0.45362078 0.23360927 0.31276996]\n",
      " [0.3695646  0.29438625 0.33604914]\n",
      " [0.21821758 0.33880905 0.44297337]\n",
      " [0.36714924 0.29180974 0.34104102]\n",
      " [0.42520741 0.36985653 0.20493606]] \n",
      "\n",
      "Output-wise summations: [1. 1. 1. 1. 1.] \n",
      "\n",
      "The two softmaxes are close but not equal!\n",
      "Cause for concern: Numerical instability.\n"
     ]
    }
   ],
   "source": [
    "normalized = x - np.max(x, axis=-1).reshape(-1,1)\n",
    "softmax_norml = softmax_func(normalized)\n",
    "softmax_unrml = softmax_func(x)\n",
    "\n",
    "print(softmax_norml, \"\\n\")\n",
    "print(softmax_unrml, \"\\n\")\n",
    "print('Output-wise summations:', np.sum(softmax_unrml, axis=1), \"\\n\")\n",
    "\n",
    "if np.allclose(softmax_norml, softmax_unrml) and False in (softmax_norml == softmax_unrml):\n",
    "    print('The two softmaxes are close but not equal!')\n",
    "    print('Cause for concern: Numerical instability.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Staged Softmax\n",
    "\n",
    "Instead of computing the softmax derivative, we [previously](../4-Classification/classification.ipynb) used the fact that<br>\n",
    "the derivative of the softmax and negative log likelihood functions \"cancel out\", and that we end up with a derivative of $p-y$.\n",
    "\n",
    "But what if we want to find the uncancelled derivative ourselves?\n",
    "\n",
    "We can approach this analytically, and find the derivative of the entire function.<br>\n",
    "We can even use SymPy to help us do the derivation, like we did in an [earlier lesson](../4-Classification/classification.ipynb).<br>\n",
    "Another method is to break the softmax function apart into individual operations.\n",
    "\n",
    "Again, this is what softmax looks like:\n",
    "$$\\zeta=\\frac{e^{\\hat{y_{i}}}}{\\sum_{j=0}e^{\\hat{y_{j}}}}$$\n",
    "\n",
    "Each operation will make a single modification to the data:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/comp_graph/softmax_steps.svg\" alt=\"Softmax Steps\">\n",
    "</div>\n",
    "\n",
    "We perform $3$ operations on the data:\n",
    "- $\\text{Exp}$ - we raise $e$ to the power $x$.\n",
    "- $\\text{Sum}$ - we add up the $e^x$ values for each row.\n",
    "- $\\text{Divide}$ - we divide the $e^x$ values by the sums.\n",
    "\n",
    "Note that the output of $\\text{Exp}$ is passed to both the $\\text{Sum}$ and $\\text{Divide}$ operations.\n",
    "\n",
    "By breaking up the softmax this way, we can take the derivative of each individual piece instead of the whole function at once.<br>\n",
    "By applying the [chain rule](https://www.khanacademy.org/math/ap-calculus-ab/ab-differentiation-2-new/ab-3-1a/a/chain-rule-review), multiplying the derivative of each individual operation will result in the derivative of the whole function.<br>\n",
    "We used the chain rule in previous lessons to find the partial derivative of the loss with respect to the model weights and biases.\n",
    "\n",
    "Now we can build the forward pass of our staged softmax.<br>\n",
    "The derivative of multiplication is easier to calculate than division, so we'll swap some of our operations to remove the division.<br>\n",
    "Luckily for us, raising a value $x$ to the power $-1$ is the same as taking $\\frac{1}{x}$.<br>\n",
    "Instead of dividing $\\frac{\\text{Exp}}{\\text{Sum}}$, we can do $\\text{Exp}*\\text{Sum}^{-1}$.\n",
    "\n",
    "The change in notation leaves us with these operations:\n",
    "- $\\text{Exp}$\n",
    "- $\\text{Sum}$\n",
    "- $\\text{Pow}$ - we invert the sum by raising to the power $-1$\n",
    "- $\\text{Multiply}$ - we multiply the inverted sum and the exp values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.45362078 0.23360927 0.31276996]\n",
      " [0.3695646  0.29438625 0.33604914]\n",
      " [0.21821758 0.33880905 0.44297337]\n",
      " [0.36714924 0.29180974 0.34104102]\n",
      " [0.42520741 0.36985653 0.20493606]]\n"
     ]
    }
   ],
   "source": [
    "# Softmax, explicitly in stages\n",
    "raised = np.exp(normalized)\n",
    "summed = np.sum(raised, axis=-1).reshape(-1,1) # Reshape so each row has 1 column.\n",
    "pow = summed ** -1\n",
    "staged_softmax = raised * pow\n",
    "\n",
    "print(staged_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our staged softmax has the same output as our original, normalized function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Derivative\n",
    "\n",
    "To get the softmax derivative, we need to progrss through the operations we did before, but backwards:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/comp_graph/softmax_steps_full_bwd.svg\" alt=\"Softmax Steps Full\">\n",
    "</div>\n",
    "\n",
    "To calculate the loss of our softmaxed output, we will use the negative log likelihood.<br>\n",
    "The negative log likelihood is $NLL = -\\sum_{i=0} y_{i} \\log p_{i}$.\n",
    "\n",
    "$y$ is only non-zero at a single position per row.<br>\n",
    "This means, we will only have a single-valued $NLL$ result (the $-y_{i} * \\log p_{i}$ where $i$ is the correct label at $y=1$).\n",
    "\n",
    "We can solve this derivative by breaking $NLL$ into two steps:\n",
    "- $\\log p_{i}$ - the derivative of $\\text{log}_e(p_{i})$ is $\\frac{1}{p_{i}}$\n",
    "- $-y_{i} * \\log p_{i}$ - the derivative w.r.t $\\log p_{i}$ is $(-y_{i})$\n",
    "\n",
    "So the derivative of $NLL$ is $\\frac{1}{p_{i}} * (-y_{i}) = \\frac{-y_{i}}{p_{i}}$.<br>\n",
    "We'll use this negative log likelihood derivative $\\frac{\\partial L}{\\partial p}$ below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.20448456 -0.         -0.        ]\n",
      " [-2.70588685 -0.         -0.        ]\n",
      " [-4.5825822  -0.         -0.        ]\n",
      " [-2.72368804 -0.         -0.        ]\n",
      " [-2.35179343 -0.         -0.        ]]\n"
     ]
    }
   ],
   "source": [
    "nll_grad = lambda y, pred: -1 * y / pred\n",
    "\n",
    "loss_grad = nll_grad(y, staged_softmax)\n",
    "print(loss_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the softmax derivative by multiplying the $NLL$'s derivatives of the individual operations.\n",
    "\n",
    "- $\\text{Exp}$ - the derivative of $e^x$ is $e^x$\n",
    "- $\\text{Sum}$ - as a sum operation will combine input elements into one, we just distribute the gradient over all input elements. A change to any of the input elements will have a direct impact on the output.\n",
    "- $\\text{Pow}$ - the derivative of $x^{-1}$ is $-1 * x^{-2}$. More on this [here](https://www.khanacademy.org/math/old-ap-calculus-ab/ab-derivative-rules/ab-diff-negative-fraction-powers/a/power-rule-review).\n",
    "- $\\text{Multiply}$ - the derivatives of $x*y$ are $y$ wrt $x$ and $x$ wrt $y$.  This is because any change to $x$ is multiplied by $y$, and vice versa. Thus the rate of change of $x$ is $y$, and vice versa.\n",
    "\n",
    "Let's build up the backward pass of our staged softmax. The backward pass will start with the loss gradient.<br>\n",
    "This will be a matrix showing how much we need to adjust each of the output values from our softmax to reduce our loss.<br>\n",
    "We can then compute gradients for each operation, ending with the gradient against the input, $x$.<br>\n",
    "If $x$ was the output of a neural network, we would continue backpropagation at that point to adjust the network parameters.\n",
    "\n",
    "We'll name each gradient according to the step it is a gradient for, not the step it is coming from.<br>\n",
    "So `raised_grad` is the gradient on `raised`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Backpropagation through the softmax\n",
    "\n",
    "# Step 4\n",
    "# staged_softmax = raised * pow -> pow is partial derivative for raised, concat with loss_grad\n",
    "raised_grad = loss_grad * pow\n",
    "pow_grad = loss_grad * raised\n",
    "pow_grad = np.sum(pow_grad, axis=-1).reshape(-1,1) # reshape gradient to match input data\n",
    "\n",
    "# Step 3\n",
    "# pow = summed ** -1 -> derivative of summed is -1 * summed ** -2, concat with pow_grad\n",
    "summed_grad = (-1 * summed ** -2) * pow_grad\n",
    "\n",
    "# Step 2\n",
    "# raised = np.exp(normalized) -> exp is applied to each element individually\n",
    "raised_grad_2 = np.ones_like(raised) * summed_grad # concat + distribute gradient across inputs\n",
    "\n",
    "# Step 1\n",
    "# summed = np.sum(raised, axis=-1).reshape(-1,1) -> Add up all the raised values\n",
    "raised_grad += raised_grad_2 # sum incoming gradients\n",
    "# Finally, concatenate raised_grad with the derivative of exp(normalized)\n",
    "staged_softmax_grad = raised_grad * np.exp(normalized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did two things above that might seem confusing.\n",
    "\n",
    "The first is that we summed $2$ gradients on `raised`.<br>\n",
    "This is because `raised` connects to $2$ operations, and both have separate gradients.<br>\n",
    "**Whenever this happens, we sum the gradients.**\n",
    "\n",
    "The second is that we reshaped `pow_grad` to have a single column.<br>\n",
    "This is to match `pow`, which only had `1` column in the forward pass.<br>\n",
    "**Whenever a gradient doesn't match the shape of the input data, we change the size of the gradient to match it.**<br>\n",
    "This is because the gradient represents the partial derivative against the input data to the operation.\n",
    "\n",
    "\n",
    "\n",
    "We can compare our result to the derivative of the softmax equation to make sure everything worked.<br>\n",
    "The derivative of the softmax is $S_{i}((i==j) - S_{j})$.  We take each element of a single row in the output of a softmax, like this:\n",
    "\n",
    "    [0.28, 0.25, 0.47]\n",
    "\n",
    "We then compare each element against each other element. So we could start at element $0$ ($.28$), and compare it to itself.<br>\n",
    "Then $i$ is $0$ and $j$ is $0$. So the equation is $.28 * (1 - .28)$.  When we then keep $i$ the same, but move $j$ to $1$.<br>\n",
    "The equation becomes $.28 * (0 - .25)$.  \n",
    "\n",
    "And so on, until we construct a matrix like this:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/comp_graph/softmax_deriv.svg\" alt=\"Softmax Derivative\">\n",
    "</div>\n",
    "\n",
    "We then sum across the rows and multiply by the incoming gradient to get the partial derivative against the inputs.<br>\n",
    "We can define the softmax derivative in code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def softmax_grad_func(softmax, loss_grad):\n",
    "    output = np.zeros_like(softmax)\n",
    "    for i in range(softmax.shape[0]):\n",
    "        sm_row = softmax[i,:]\n",
    "        sm_grad = (-np.outer(sm_row, sm_row) + np.diag(sm_row.flatten()))\n",
    "        row_grad = sm_grad * loss_grad[i,:].reshape(1,-1)\n",
    "        output[i,:] = np.sum(row_grad, -1)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then compare our derivative by stages with the derivation.<br>\n",
    "The `np.allclose` function tells us if all the values in an array are close to another array.<br>\n",
    "We use this instead of `==` because there are small numerical differences in similar computations with numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "derived_softmax_grad = softmax_grad_func(staged_softmax, loss_grad)\n",
    "np.allclose(derived_softmax_grad, staged_softmax_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also compare the gradient we computed of the loss and softmax together with the derivative of both together, which is $p-y$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(staged_softmax_grad, staged_softmax - y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations\n",
    "\n",
    "Breaking softmax into stages helped greatly in understanding the basic units of a computational graph.<br>\n",
    "But what if we don't want to have to type out all the code for the forward and backward pass every time?\n",
    "\n",
    "It would be nice if we could only define the forward pass, and automatically have the backward pass happen. The solution would be an autograd engine. Frameworks like PyTorch provide exactly that.\n",
    "\n",
    "We can build our own version of PyTorch by individually defining each operation, then mixing and matching the operations to create a more complex equation.<br>\n",
    "By doing this, we can create a computational graph like this one:\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"../images/comp_graph/comp_graph.png\" alt=\"Computational Graph\">\n",
    "</div>\n",
    "\n",
    "Each node in the graph will be a separate class that knows how to do a forward and backward pass.<br>\n",
    "So, we can just execute the graph to run forward and backward passes.\n",
    "\n",
    "We can start out by defining the operations.<br>\n",
    "I've written a class called `Node`, which we can subclass to define each operation. You can look at the code for `Node` if you want.<br>\n",
    "It gives us some nice methods for running the operations in a graph in order, both forward and backward:\n",
    "\n",
    "- `apply_fwd` - runs the forward pass up to the node it is called on.\n",
    "- `apply_bwd` - runs the backward pass from the node it is called on backwards.\n",
    "- `zero_grad` - zero out our gradient before running a backward pass.\n",
    "- `generate_graph` - helps us visualize the computational graph.\n",
    "- `generate_derivative_chains` - shows us the equation for calculating the partial derivative at a node.\n",
    "\n",
    "For each operation, we just implement the `forward` and `backward` methods, which take in input data, and pass them through the operation.<br>\n",
    "**The `Node` class from there on takes care of the rest.**\n",
    "\n",
    "We'll use the same formulas for each operation that we used in our staged softmax earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Exp(Node):\n",
    "    def forward(self, x):\n",
    "        return np.exp(x) # raise e to the power x\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0] # Pull the x value used in the forward pass\n",
    "        return np.exp(x) * grad # multiply the incoming gradient by the derivative\n",
    "\n",
    "class Sum(Node):\n",
    "    def forward(self, x):\n",
    "        return np.sum(x, axis=-1).reshape(-1,1)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0] # Pull the x value used in the forward pass\n",
    "        return np.ones_like(x) * grad # distribute the gradient over the input data shape\n",
    "\n",
    "class Pow(Node):\n",
    "    def forward(self, x, exponent):\n",
    "        return x ** exponent\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, exponent = self.cache # Pull the x and exponent values used in the forward pass\n",
    "        return grad * exponent * x ** (exponent - 1), 1\n",
    "\n",
    "class Multiply(Node):\n",
    "    def forward(self, x, y):\n",
    "        return x * y\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, y = self.cache # Pull the x and y values used in the forward pass\n",
    "        return grad * y, grad * x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, we've defined the $4$ operations that we need for our softmax in code.<br>\n",
    "We can now define our whole softmax operation as a computational graph.<br>\n",
    "When we initialize a `Node`, we pass in the nodes that feed into it.\n",
    "\n",
    "If we're using the node to feed in data (like inputs), we use a special `Parameter` node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# The input data for our network.  We set needs_grad=True so the gradient is calculated for this parameter in the backward pass.\n",
    "# Desc is a short description of what the data in this node is.\n",
    "X = Parameter(x, desc=\"X\", needs_grad=True)\n",
    "\n",
    "# Raise e to the power x.  Out is a description of the output of the node.\n",
    "raised = Exp(X, out=\"e^X\")\n",
    "# Sum the raised values.\n",
    "summed = Sum(raised, out=\"sum(e^X)\")\n",
    "\n",
    "# Define -1 as a parameter, so we can use it as an exponent.\n",
    "negative_one = Parameter(-1, desc=\"-1\", needs_grad=False)\n",
    "# Invert our sums\n",
    "inverted = Pow(summed, negative_one, out=\"1 / sum(e^X)\")\n",
    "# Multiply the inverted sums by e^X\n",
    "softmax = Multiply(raised, inverted, out=\"softmax(X)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the `generate_graph` method on the `softmax` node to visualize the computational graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: fwd_pass Pages: 1 -->\n",
       "<svg width=\"614pt\" height=\"128pt\"\n",
       " viewBox=\"0.00 0.00 614.09 128.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 124)\">\n",
       "<title>fwd_pass</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-124 610.09,-124 610.09,4 -4,4\"/>\n",
       "<!-- 2796083493872 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2796083493872</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"563.84\" cy=\"-18\" rx=\"42.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"563.84\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Multiply</text>\n",
       "</g>\n",
       "<!-- 2796081214912 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2796081214912</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"127\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"127\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Exp</text>\n",
       "</g>\n",
       "<!-- 2796081214912&#45;&gt;2796083493872 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2796081214912&#45;&gt;2796083493872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M154.22,-18C224.48,-18 417.96,-18 511.24,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"511.29,-21.5 521.29,-18 511.29,-14.5 511.29,-21.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.1\" y=\"-21.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">e^X</text>\n",
       "</g>\n",
       "<!-- 2796083772160 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2796083772160</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"240.3\" cy=\"-102\" rx=\"27.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.3\" y=\"-98.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Sum</text>\n",
       "</g>\n",
       "<!-- 2796081214912&#45;&gt;2796083772160 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2796081214912&#45;&gt;2796083772160</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M145.88,-31.5C164.14,-45.27 192.66,-66.8 213.38,-82.44\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"211.37,-85.31 221.46,-88.54 215.59,-79.72 211.37,-85.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"183.5\" y=\"-70.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">e^X</text>\n",
       "</g>\n",
       "<!-- 2796083029600 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2796083029600</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 2796083029600&#45;&gt;2796081214912 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2796083029600&#45;&gt;2796081214912</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M54,-18C64.97,-18 77.92,-18 89.79,-18\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"89.87,-21.5 99.87,-18 89.87,-14.5 89.87,-21.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"77\" y=\"-21.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 2796083490656 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2796083490656</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"385.6\" cy=\"-48\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"385.6\" y=\"-44.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Pow</text>\n",
       "</g>\n",
       "<!-- 2796083490656&#45;&gt;2796083493872 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2796083490656&#45;&gt;2796083493872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M411.85,-43.7C438.5,-39.16 481.17,-31.9 514.36,-26.25\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"515.05,-29.68 524.33,-24.56 513.88,-22.78 515.05,-29.68\"/>\n",
       "<text text-anchor=\"middle\" x=\"467.1\" y=\"-42.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">1 / sum(e^X)</text>\n",
       "</g>\n",
       "<!-- 2796083772160&#45;&gt;2796083490656 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2796083772160&#45;&gt;2796083490656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M265.14,-94.11C285.36,-87.3 315.06,-77 340.6,-67 344.81,-65.35 349.21,-63.55 353.54,-61.73\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"354.97,-64.92 362.79,-57.77 352.21,-58.49 354.97,-64.92\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.1\" y=\"-89.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">sum(e^X)</text>\n",
       "</g>\n",
       "<!-- 2796083772544 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2796083772544</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"240.3\" cy=\"-48\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"240.3\" y=\"-44.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">&#45;1</text>\n",
       "</g>\n",
       "<!-- 2796083772544&#45;&gt;2796083490656 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2796083772544&#45;&gt;2796083490656</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M267.58,-48C290.2,-48 323.08,-48 348.23,-48\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"348.48,-51.5 358.48,-48 348.48,-44.5 348.48,-51.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"313.1\" y=\"-51.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">&#45;1</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x28b03892510>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the `apply_fwd` method to calculate the softmax.<br>\n",
    "We can then verify that it is the same as our `staged_softmax` that we calculated earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "operations_softmax = softmax.apply_fwd()\n",
    "np.allclose(staged_softmax, operations_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now, for the cool part.**<br>\n",
    "We can use the same computational graph to do the backward pass!\n",
    "\n",
    "Let's first visualize what the backward pass looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: fwd_pass Pages: 1 -->\n",
       "<svg width=\"678pt\" height=\"139pt\"\n",
       " viewBox=\"0.00 0.00 678.09 139.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 135)\">\n",
       "<title>fwd_pass</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-135 674.09,-135 674.09,4 -4,4\"/>\n",
       "<!-- 2796083493872 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2796083493872</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"42.25\" cy=\"-95\" rx=\"42.49\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"42.25\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Multiply</text>\n",
       "</g>\n",
       "<!-- 2796081214912 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2796081214912</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"527.09\" cy=\"-95\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"527.09\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Exp</text>\n",
       "</g>\n",
       "<!-- 2796083493872&#45;&gt;2796081214912 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2796083493872&#45;&gt;2796081214912</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M81.71,-101.7C88.61,-102.66 95.75,-103.49 102.49,-104 270.72,-116.7 314.24,-120.99 482.09,-104 485.17,-103.69 488.35,-103.26 491.53,-102.75\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"492.34,-106.16 501.55,-100.93 491.09,-99.27 492.34,-106.16\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.99\" y=\"-119.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(e^X)</text>\n",
       "</g>\n",
       "<!-- 2796083490656 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2796083490656</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"236.49\" cy=\"-72\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"236.49\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Pow</text>\n",
       "</g>\n",
       "<!-- 2796083493872&#45;&gt;2796083490656 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2796083493872&#45;&gt;2796083490656</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M83.08,-90.23C117.42,-86.13 166.55,-80.25 199.67,-76.29\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"200.3,-79.73 209.82,-75.07 199.47,-72.78 200.3,-79.73\"/>\n",
       "<text text-anchor=\"middle\" x=\"146.99\" y=\"-91.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(1 / sum(e^X))</text>\n",
       "</g>\n",
       "<!-- 2796083029600 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2796083029600</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"643.09\" cy=\"-95\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"643.09\" y=\"-91.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 2796081214912&#45;&gt;2796083029600 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2796081214912&#45;&gt;2796083029600</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M554.24,-95C569.59,-95 589.22,-95 606.02,-95\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"606.06,-98.5 616.06,-95 606.06,-91.5 606.06,-98.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"585.09\" y=\"-98.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(X)</text>\n",
       "</g>\n",
       "<!-- 2796083772160 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2796083772160</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"397.79\" cy=\"-72\" rx=\"27.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"397.79\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Sum</text>\n",
       "</g>\n",
       "<!-- 2796083490656&#45;&gt;2796083772160 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2796083490656&#45;&gt;2796083772160</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M263.8,-72C290.08,-72 330.72,-72 360.16,-72\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"360.45,-75.5 370.45,-72 360.45,-68.5 360.45,-75.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"316.99\" y=\"-75.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(sum(e^X))</text>\n",
       "</g>\n",
       "<!-- 2796083772544 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2796083772544</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"397.79\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"397.79\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">&#45;1</text>\n",
       "</g>\n",
       "<!-- 2796083490656&#45;&gt;2796083772544 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2796083490656&#45;&gt;2796083772544</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M261,-64.03C288.13,-54.83 332.96,-39.64 363.63,-29.24\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"365.1,-32.44 373.45,-25.91 362.85,-25.81 365.1,-32.44\"/>\n",
       "</g>\n",
       "<!-- 2796083772160&#45;&gt;2796081214912 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2796083772160&#45;&gt;2796081214912</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M424.24,-76.6C443.32,-80.05 469.67,-84.81 490.79,-88.62\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"490.21,-92.08 500.68,-90.41 491.46,-85.19 490.21,-92.08\"/>\n",
       "<text text-anchor=\"middle\" x=\"462.59\" y=\"-90.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(e^X)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x28b03892b10>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph(backward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then run `apply_bwd` to run the backward pass.<br>\n",
    "We first call `zero_grad` to ensure that the initial gradients on all nodes are properly set to zero.\n",
    "\n",
    "Any parameter nodes with `needs_grad` set to `True` will now have a property called `grad`, where we can get the gradient.<br>\n",
    "We can get the gradient from `X`, and verify that it matches our staged gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.zero_grad()\n",
    "softmax.apply_bwd(loss_grad)\n",
    "\n",
    "operations_softmax_grad = X.grad\n",
    "np.allclose(staged_softmax_grad, operations_softmax_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a bonus, we can also check the equations that we multiplied to calculate the partial derivative with respect to `X`.\n",
    "\n",
    "You can see that we had to add together 2 different gradients, just like we did with the staged version.<br>\n",
    "The $\\partial$ symbol means partial derivative, and $\\frac{\\partial e^{X}}{\\partial X}$ means \"the partial derivative of $e^X$ with respect to $X$.<br>\n",
    "The way to interpret this partial derivative is \"as $X$ changes, how does $e^{X}$ change?\".<br>\n",
    "By multiplying the partial derivatives of each operation, we can get the larger partial derivative, which is $\\frac{\\partial L}{\\partial X}$ - how the loss changes as we change $X$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\frac{\\partial}{\\partial X} = \\frac{\\partial softmax(X)}{\\partial e^X}*\\frac{\\partial e^X}{\\partial X} + \\\\\\frac{\\partial softmax(X)}{\\partial 1 / sum(e^X)}*\\frac{\\partial 1 / sum(e^X)}{\\partial sum(e^X)}*\\frac{\\partial sum(e^X)}{\\partial e^X}*\\frac{\\partial e^X}{\\partial X}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_derivative_chains()\n",
    "display_chain(X.display_partial_derivative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just built a computational graph of the softmax function, then ran a forward and backward pass.<br>\n",
    "This is exactly how deep learning frameworks like PyTorch and TensorFlow work.<br>\n",
    "They define common operators, like `torch.dot`, keep track of what operators you called, build a graph,<br>\n",
    "and automatically run the backward pass based on the derivative of each operator.<br>\n",
    "You could say we built a miniature deep learning framework!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-Layer Network\n",
    "\n",
    "Let's extend our framework to work across a 2-Layer neural network.<br>\n",
    "We need to define 3 additional operations:\n",
    "\n",
    "- `MatMul` - to multiply two matrices. The forward pass is `x@w`.\n",
    "- `Add` - add two values up.\n",
    "- `Relu` - a nonlinear activation function. Anything below $0$ will be set to $0$.\n",
    "\n",
    "Here are the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class MatMul(Node):\n",
    "    def forward(self, x, w):\n",
    "        return x @ w # multiply the two matrices.\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, w = self.cache\n",
    "        # return the input gradient times the weights as the gradient on x\n",
    "        # the input x values (from the forward pass) times the input gradient is the gradient on the weights\n",
    "        return grad @ w.T, x.T @ grad\n",
    "\n",
    "\n",
    "class Add(Node):\n",
    "    def forward(self, x, b):\n",
    "        return x + b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        # Any change to x or b will scale the output the same amount\n",
    "        return grad, grad\n",
    "\n",
    "\n",
    "class Relu(Node):\n",
    "    def forward(self, x):\n",
    "        return np.maximum(x, 0)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0]\n",
    "        new_grad = np.array(grad)\n",
    "        # The derivative of relu is 0 when the input\n",
    "        # in the forward pass was below 0\n",
    "        # 1 otherwise\n",
    "        new_grad[x < 0] = 0\n",
    "        return new_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you train a neural network using a framework like PyTorch, you will usually be using the GPU.<br>\n",
    "GPUs enable us to parallelize operations and train neural networks much faster than we could with a CPU.\n",
    "\n",
    "One way to speed up GPU code is to fuse operators.<br>\n",
    "This means that we combine operators that run together into a single operator.<br>\n",
    "This combined operator can then be optimized into a kernel that runs better on the GPU.<br>\n",
    "We'll discuss this more in a future lesson.\n",
    "\n",
    "As an example of the process, below you can see the fusion of the `Softmax` operation.<br>\n",
    "The fused softmax used the softmax functions we defined before to do the forward and backward passes.\n",
    "\n",
    "We also fuse an entire neural network layer into the `Dense` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class Softmax(Node):\n",
    "    def forward(self, x):\n",
    "        return softmax_func(x)\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x = self.cache[0]\n",
    "        softmax = self.forward(x)\n",
    "        return softmax_grad_func(softmax, grad)\n",
    "\n",
    "class Dense(Node):\n",
    "    def forward(self, x, w, b):\n",
    "        # Multiply by weight, add bias\n",
    "        return x @ w + b\n",
    "\n",
    "    def backward(self, grad):\n",
    "        x, w, b = self.cache\n",
    "        # Return 3 gradients for x, w, and b\n",
    "        return grad @ w.T, x.T @ grad, grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have the operators we need to define our neural network. Let's first load in the data.<br>\n",
    "We'll use the same telescope data from an earlier lesson.<br>\n",
    "We have observations from a telescope, and we want to classify whether each observation is a star, galaxy, or quasar.\n",
    "\n",
    "We use a data wrapper that I wrote to load and split the data automatically.<br>\n",
    "We'll only use the training set in this lesson, but you can experiment with the other $2$ sets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.16745842 -0.58492272  1.03148637 -0.34855938 -0.83728027 -0.94605772\n",
      "  -0.99534154 -0.83806089  0.21085172 -0.21763043 -0.36973112  1.03148936\n",
      "   1.30931064]\n",
      " [ 0.16886159 -0.58311429  0.05243046 -0.16653251 -0.15415531 -0.08264457\n",
      "  -0.02604308 -0.83806089  0.21085172 -0.21763043 -0.36984929 -0.63621258\n",
      "  -0.87919741]]\n"
     ]
    }
   ],
   "source": [
    "# Load the data\n",
    "wrapper = SkyServerDatasetWrapper()\n",
    "[train_x, train_y], [valid_x, valid_y], [test_x, test_y] = wrapper.get_flat_datasets()\n",
    "\n",
    "print(train_x[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then need to one-hot encode our data so that we can use it for classification.<br>\n",
    "We create $3$-element vectors where only one element is $1$, and the others are $0$.<br>\n",
    "The position of the $1$ corresponds to the target we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def encode(target, max_value=3):\n",
    "    # A matrix with 3 columns\n",
    "    encoded = np.zeros((target.shape[0], max_value))\n",
    "    # Setup the indices that we'll set to one\n",
    "    inds = target.astype(int)\n",
    "    # Set the target positions to 1\n",
    "    encoded[inds] = 1\n",
    "    return encoded\n",
    "\n",
    "train_y = encode(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our training data, we can initialize our weights and biases.<br>\n",
    "We're taking in $13$ features, so we'll setup our weights for the first layer accordingly.<br>\n",
    "We want to output $3$ digits, one for each encoding position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Set a seed so we can reproduce results\n",
    "np.random.seed(0)\n",
    "\n",
    "w1 = np.random.rand(13, 10)\n",
    "b1 = np.random.rand(1, 10)\n",
    "w2 = np.random.rand(10, 3)\n",
    "b2 = np.random.rand(1, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define our network in code!<br>\n",
    "This is very similar to networks we've built in the past.<br>\n",
    "We have our first layer, with a ReLU activation, then our second layer, then Softmax to get probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "X = Parameter(train_x, desc=\"X\", needs_grad=False)\n",
    "Y = Parameter(train_y, desc=\"y\", needs_grad=False)\n",
    "\n",
    "w1_param = Parameter(w1, desc=\"W1\")\n",
    "b1_param = Parameter(b1, desc=\"b1\")\n",
    "\n",
    "matmul1 = MatMul(X, w1_param, out=\"X @ W1\")\n",
    "add1 = Add(matmul1, b1_param, out=\"Z1\")\n",
    "\n",
    "layer1 = Relu(add1, out=\"A1\")\n",
    "\n",
    "w2_param = Parameter(w2, desc=\"W2\")\n",
    "b2_param = Parameter(b2, desc=\"b2\")\n",
    "matmul2 = MatMul(layer1, w2_param, out=\"Z1 @ W2\")\n",
    "add2 = Add(matmul2, b2_param, out=\"Z2\")\n",
    "\n",
    "softmax = Softmax(add2, out=\"softmax(Z2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like before, we can create a computation graph.  This one is more complex than before, but should still be readable!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: fwd_pass Pages: 1 -->\n",
       "<svg width=\"863pt\" height=\"191pt\"\n",
       " viewBox=\"0.00 0.00 862.67 191.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>fwd_pass</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-187 858.67,-187 858.67,4 -4,4\"/>\n",
       "<!-- 2796296506256 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2796296506256</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"813.73\" cy=\"-37\" rx=\"40.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"813.73\" y=\"-33.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Softmax</text>\n",
       "</g>\n",
       "<!-- 2796296511872 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2796296511872</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"693.78\" cy=\"-37\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"693.78\" y=\"-33.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Add</text>\n",
       "</g>\n",
       "<!-- 2796296511872&#45;&gt;2796296506256 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2796296511872&#45;&gt;2796296506256</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M720.96,-37C733.19,-37 748.18,-37 762.48,-37\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"762.71,-40.5 772.71,-37 762.71,-33.5 762.71,-40.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"746.78\" y=\"-40.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z2</text>\n",
       "</g>\n",
       "<!-- 2796296505152 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2796296505152</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"535.48\" cy=\"-72\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"535.48\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MatMul</text>\n",
       "</g>\n",
       "<!-- 2796296505152&#45;&gt;2796296511872 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2796296505152&#45;&gt;2796296511872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M571.95,-64.06C597.6,-58.31 632.09,-50.59 657.66,-44.86\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"658.68,-48.22 667.67,-42.62 657.15,-41.39 658.68,-48.22\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.28\" y=\"-61.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z1 @ W2</text>\n",
       "</g>\n",
       "<!-- 2796296504240 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2796296504240</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"410.89\" cy=\"-103\" rx=\"27.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"410.89\" y=\"-99.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Relu</text>\n",
       "</g>\n",
       "<!-- 2796296504240&#45;&gt;2796296505152 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2796296504240&#45;&gt;2796296505152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M436.7,-96.73C452.05,-92.85 472.2,-87.75 490.21,-83.2\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"491.12,-86.58 499.96,-80.73 489.4,-79.79 491.12,-86.58\"/>\n",
       "<text text-anchor=\"middle\" x=\"466.69\" y=\"-94.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">A1</text>\n",
       "</g>\n",
       "<!-- 2796296504432 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2796296504432</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"304.59\" cy=\"-103\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"304.59\" y=\"-99.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Add</text>\n",
       "</g>\n",
       "<!-- 2796296504432&#45;&gt;2796296504240 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2796296504432&#45;&gt;2796296504240</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M331.64,-103C344.24,-103 359.57,-103 373.32,-103\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"373.45,-106.5 383.45,-103 373.45,-99.5 373.45,-106.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"357.59\" y=\"-106.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Z1</text>\n",
       "</g>\n",
       "<!-- 2796296516720 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2796296516720</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"151.3\" cy=\"-138\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.3\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MatMul</text>\n",
       "</g>\n",
       "<!-- 2796296516720&#45;&gt;2796296504432 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2796296516720&#45;&gt;2796296504432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M187.39,-129.88C211.83,-124.23 244.29,-116.72 268.69,-111.07\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"269.66,-114.44 278.61,-108.78 268.08,-107.62 269.66,-114.44\"/>\n",
       "<text text-anchor=\"middle\" x=\"234.59\" y=\"-127.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X @ W1</text>\n",
       "</g>\n",
       "<!-- 2796296508752 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2796296508752</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-165\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-161.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 2796296508752&#45;&gt;2796296516720 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2796296508752&#45;&gt;2796296516720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.75,-159.54C67.68,-156.24 87.18,-151.94 104.81,-148.04\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"105.91,-151.38 114.92,-145.81 104.4,-144.55 105.91,-151.38\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.5\" y=\"-157.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 2796296515568 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2796296515568</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"27\" cy=\"-111\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"27\" y=\"-107.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">W1</text>\n",
       "</g>\n",
       "<!-- 2796296515568&#45;&gt;2796296516720 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2796296515568&#45;&gt;2796296516720</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M52.75,-116.46C67.68,-119.76 87.18,-124.06 104.81,-127.96\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"104.4,-131.45 114.92,-130.19 105.91,-124.62 104.4,-131.45\"/>\n",
       "<text text-anchor=\"middle\" x=\"82.5\" y=\"-128.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">W1</text>\n",
       "</g>\n",
       "<!-- 2796296510096 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2796296510096</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"151.3\" cy=\"-84\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"151.3\" y=\"-80.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b1</text>\n",
       "</g>\n",
       "<!-- 2796296510096&#45;&gt;2796296504432 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2796296510096&#45;&gt;2796296504432</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M178.43,-85.17C200.24,-86.39 232.07,-88.74 259.59,-93 262.67,-93.48 265.85,-94.04 269.03,-94.67\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"268.53,-98.14 279.04,-96.78 269.98,-91.29 268.53,-98.14\"/>\n",
       "<text text-anchor=\"middle\" x=\"234.59\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b1</text>\n",
       "</g>\n",
       "<!-- 2796296507984 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2796296507984</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"410.89\" cy=\"-49\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"410.89\" y=\"-45.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">W2</text>\n",
       "</g>\n",
       "<!-- 2796296507984&#45;&gt;2796296505152 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2796296507984&#45;&gt;2796296505152</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M437.73,-51.94C449.77,-53.47 464.26,-55.54 477.19,-58 481.55,-58.83 486.08,-59.78 490.59,-60.79\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"489.84,-64.21 500.37,-63.06 491.42,-57.39 489.84,-64.21\"/>\n",
       "<text text-anchor=\"middle\" x=\"466.69\" y=\"-61.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">W2</text>\n",
       "</g>\n",
       "<!-- 2796296509280 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2796296509280</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"535.48\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"535.48\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b2</text>\n",
       "</g>\n",
       "<!-- 2796296509280&#45;&gt;2796296511872 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2796296509280&#45;&gt;2796296511872</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M562.48,-19.02C585.31,-20.18 619.39,-22.51 648.78,-27 651.86,-27.47 655.04,-28.04 658.22,-28.65\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"657.73,-32.13 668.23,-30.77 659.17,-25.28 657.73,-32.13\"/>\n",
       "<text text-anchor=\"middle\" x=\"621.28\" y=\"-30.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b2</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x28b104290a0>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create predictions using `apply_fwd`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.51408579, 0.29557176, 0.19034245],\n",
       "       [0.42508635, 0.26655849, 0.30835516]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = softmax.apply_fwd()\n",
    "\n",
    "predictions[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can run the backward pass of the network.  We'll first graph it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.50.0 (0)\n",
       " -->\n",
       "<!-- Title: fwd_pass Pages: 1 -->\n",
       "<svg width=\"957pt\" height=\"191pt\"\n",
       " viewBox=\"0.00 0.00 956.67 191.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 187)\">\n",
       "<title>fwd_pass</title>\n",
       "<polygon fill=\"white\" stroke=\"transparent\" points=\"-4,4 -4,-187 952.67,-187 952.67,4 -4,4\"/>\n",
       "<!-- 2796296506256 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>2796296506256</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"40.95\" cy=\"-37\" rx=\"40.89\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"40.95\" y=\"-33.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Softmax</text>\n",
       "</g>\n",
       "<!-- 2796296511872 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>2796296511872</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"175.89\" cy=\"-37\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.89\" y=\"-33.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Add</text>\n",
       "</g>\n",
       "<!-- 2796296506256&#45;&gt;2796296511872 -->\n",
       "<g id=\"edge10\" class=\"edge\">\n",
       "<title>2796296506256&#45;&gt;2796296511872</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M82.2,-37C100.04,-37 120.89,-37 138.3,-37\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"138.63,-40.5 148.63,-37 138.63,-33.5 138.63,-40.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"115.39\" y=\"-40.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(Z2)</text>\n",
       "</g>\n",
       "<!-- 2796296505152 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2796296505152</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"350.19\" cy=\"-72\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.19\" y=\"-68.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MatMul</text>\n",
       "</g>\n",
       "<!-- 2796296511872&#45;&gt;2796296505152 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>2796296511872&#45;&gt;2796296505152</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M201.95,-42.09C228.39,-47.46 270.66,-56.05 303.14,-62.65\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"302.81,-66.15 313.31,-64.71 304.2,-59.29 302.81,-66.15\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.39\" y=\"-62.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(Z1 @ W2)</text>\n",
       "</g>\n",
       "<!-- 2796296509280 -->\n",
       "<g id=\"node11\" class=\"node\">\n",
       "<title>2796296509280</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"350.19\" cy=\"-18\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"350.19\" y=\"-14.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b2</text>\n",
       "</g>\n",
       "<!-- 2796296511872&#45;&gt;2796296509280 -->\n",
       "<g id=\"edge9\" class=\"edge\">\n",
       "<title>2796296511872&#45;&gt;2796296509280</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M201.43,-30.72C207.74,-29.3 214.54,-27.94 220.89,-27 251.76,-22.44 287.16,-20.17 312.94,-19.06\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"313.16,-22.55 323.02,-18.66 312.89,-15.56 313.16,-22.55\"/>\n",
       "<text text-anchor=\"middle\" x=\"256.39\" y=\"-30.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(b2)</text>\n",
       "</g>\n",
       "<!-- 2796296504240 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>2796296504240</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"490.78\" cy=\"-103\" rx=\"27.1\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"490.78\" y=\"-99.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Relu</text>\n",
       "</g>\n",
       "<!-- 2796296505152&#45;&gt;2796296504240 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2796296505152&#45;&gt;2796296504240</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M386.52,-79.9C407.38,-84.57 433.69,-90.45 454.53,-95.11\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"454.05,-98.59 464.57,-97.36 455.58,-91.76 454.05,-98.59\"/>\n",
       "<text text-anchor=\"middle\" x=\"426.99\" y=\"-95.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(A1)</text>\n",
       "</g>\n",
       "<!-- 2796296507984 -->\n",
       "<g id=\"node10\" class=\"node\">\n",
       "<title>2796296507984</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"490.78\" cy=\"-49\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"490.78\" y=\"-45.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">W2</text>\n",
       "</g>\n",
       "<!-- 2796296505152&#45;&gt;2796296507984 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>2796296505152&#45;&gt;2796296507984</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M385.27,-62.91C392.87,-61.09 400.92,-59.34 408.49,-58 423.08,-55.42 439.29,-53.45 453.37,-52.02\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"454.05,-55.47 463.67,-51.03 453.38,-48.5 454.05,-55.47\"/>\n",
       "<text text-anchor=\"middle\" x=\"426.99\" y=\"-61.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(W2)</text>\n",
       "</g>\n",
       "<!-- 2796296504432 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>2796296504432</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"612.08\" cy=\"-103\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"612.08\" y=\"-99.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">Add</text>\n",
       "</g>\n",
       "<!-- 2796296504240&#45;&gt;2796296504432 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>2796296504240&#45;&gt;2796296504432</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M518.26,-103C534.79,-103 556.29,-103 574.44,-103\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"574.68,-106.5 584.68,-103 574.68,-99.5 574.68,-106.5\"/>\n",
       "<text text-anchor=\"middle\" x=\"551.58\" y=\"-106.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(Z1)</text>\n",
       "</g>\n",
       "<!-- 2796296516720 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>2796296516720</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"781.38\" cy=\"-138\" rx=\"40.09\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"781.38\" y=\"-134.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">MatMul</text>\n",
       "</g>\n",
       "<!-- 2796296504432&#45;&gt;2796296516720 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2796296504432&#45;&gt;2796296516720</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M638.13,-108.24C663.51,-113.55 703.37,-121.89 734.45,-128.39\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"734.11,-131.9 744.62,-130.52 735.55,-125.05 734.11,-131.9\"/>\n",
       "<text text-anchor=\"middle\" x=\"690.08\" y=\"-128.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(X @ W1)</text>\n",
       "</g>\n",
       "<!-- 2796296510096 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>2796296510096</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"781.38\" cy=\"-84\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"781.38\" y=\"-80.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">b1</text>\n",
       "</g>\n",
       "<!-- 2796296504432&#45;&gt;2796296510096 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>2796296504432&#45;&gt;2796296510096</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M637.62,-96.74C643.93,-95.32 650.73,-93.95 657.08,-93 686.24,-88.65 719.61,-86.37 744.27,-85.2\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"744.44,-88.7 754.27,-84.77 744.13,-81.71 744.44,-88.7\"/>\n",
       "<text text-anchor=\"middle\" x=\"690.08\" y=\"-96.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(b1)</text>\n",
       "</g>\n",
       "<!-- 2796296508752 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>2796296508752</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"921.67\" cy=\"-165\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"921.67\" y=\"-161.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">X</text>\n",
       "</g>\n",
       "<!-- 2796296516720&#45;&gt;2796296508752 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>2796296516720&#45;&gt;2796296508752</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M818.71,-145.09C839.39,-149.13 865.16,-154.16 885.62,-158.16\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"885,-161.6 895.48,-160.08 886.34,-154.73 885,-161.6\"/>\n",
       "</g>\n",
       "<!-- 2796296515568 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>2796296515568</title>\n",
       "<ellipse fill=\"none\" stroke=\"black\" cx=\"921.67\" cy=\"-111\" rx=\"27\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"921.67\" y=\"-107.3\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">W1</text>\n",
       "</g>\n",
       "<!-- 2796296516720&#45;&gt;2796296515568 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>2796296516720&#45;&gt;2796296515568</title>\n",
       "<path fill=\"none\" stroke=\"red\" d=\"M818.71,-130.91C839.39,-126.87 865.16,-121.84 885.62,-117.84\"/>\n",
       "<polygon fill=\"red\" stroke=\"red\" points=\"886.34,-121.27 895.48,-115.92 885,-114.4 886.34,-121.27\"/>\n",
       "<text text-anchor=\"middle\" x=\"858.17\" y=\"-129.8\" font-family=\"Times New Roman,serif\" font-size=\"14.00\">d(W1)</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.graphs.Digraph at 0x28b1042a810>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax.generate_graph(backward=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we'll run our backward pass.<br>\n",
    "We first calculate our loss gradient, then pass it into our backward pass as the incoming gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "softmax.zero_grad() # zero out the gradients\n",
    "loss_grad = nll_grad(train_y, predictions) # compute the loss gradient\n",
    "softmax.apply_bwd(loss_grad) # pass the loss gradient into the softmax, and run the backward pass\n",
    "softmax.generate_derivative_chains() # generate our derivative equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of our parameters should now have a gradient.  We'll only show `w2`, but you can look at the other parameters as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.27379929, -0.05720028, -0.21659901],\n",
       "       [ 0.47819796, -0.09990186, -0.3782961 ],\n",
       "       [ 0.        ,  0.        ,  0.        ],\n",
       "       [ 0.24295538, -0.05075658, -0.1921988 ],\n",
       "       [ 0.27254325, -0.05693788, -0.21560538]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2_param.grad[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the partial derivative chains to see how we calculate the derivative with respect to a specific parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\frac{\\partial}{\\partial W2} = \\frac{\\partial softmax(Z2)}{\\partial Z2}*\\frac{\\partial Z2}{\\partial Z1 @ W2}*\\frac{\\partial Z1 @ W2}{\\partial W2}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_chain(w2_param.display_partial_derivative())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\frac{\\partial}{\\partial W1} = \\frac{\\partial softmax(Z2)}{\\partial Z2}*\\frac{\\partial Z2}{\\partial Z1 @ W2}*\\frac{\\partial Z1 @ W2}{\\partial A1}*\\frac{\\partial A1}{\\partial Z1}*\\frac{\\partial Z1}{\\partial X @ W1}*\\frac{\\partial X @ W1}{\\partial W1}$"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_chain(w1_param.display_partial_derivative())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just built a computational graph, and used it to do the full forward and backward pass for a neural network!<br>\n",
    "If you want, you can extend this to update the parameters and train the network.<br>\n",
    "You would just need to set a learning rate, then subtract the gradient from each parameter.<br>\n",
    "You would have to set a batch size, and iterate through the data as well.\n",
    "\n",
    "This has hopefully given you a good look at how backpropagation, works, and how we compute the partial derivatives of each operation, then multiply them out.\n",
    "\n",
    "Let's do a quick verification to make sure that we did everything correctly.<br>\n",
    "We can implement the network forward and backward pass like we did in an earlier lesson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Forward pass\n",
    "l1 = train_x @ w1 + b1\n",
    "l1_activated = np.maximum(l1, 0)\n",
    "l2 = l1_activated @ w2 + b2\n",
    "probs = softmax_func(l2)\n",
    "\n",
    "# Loss\n",
    "loss_grad = nll_grad(train_y, probs)\n",
    "\n",
    "# L2 gradients\n",
    "sm_grad = softmax_grad_func(probs, loss_grad)\n",
    "l2_w_grad = l1_activated.T @ sm_grad\n",
    "l2_b_grad = sm_grad.sum(axis=0)\n",
    "\n",
    "# L1 gradients\n",
    "l1_grad = sm_grad @ w2.T\n",
    "l1_grad[l1 < 0] = 0\n",
    "l1_w_grad = train_x.T @ l1_grad\n",
    "l1_b_grad = l1_grad.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can verify that our computational graph matches the manual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(l1_w_grad, w1_param.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap-up\n",
    "\n",
    "We did **a lot** in this lesson!<br>\n",
    "We learned how to break apart a derivative into steps, then compute each step separately.<br>\n",
    "Then, we constructed a computational graph and ran the forward and backward passes.\n",
    "\n",
    "I recommend doing some experimentation with the graph, and making sure you *really* understand how everything is working.<br>\n",
    "In the next lesson, we'll use PyTorch to automatically construct the graph for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
